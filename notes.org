* The problem
Say we have a monadic interpreter for a toy language.  We want to
instrument this interpreter: modify the rules to create additional
side effects, or even change the return value.

Given the following interpreter for arithmetic expressions:

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int
  data Value = Number Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Number i)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
#+END_SRC

we want to add a new construct for pairs of integers.  The
straightforward way is to extend the existing interpreter:

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int | Couple Int Int
  data Value = Number Int | Pair Int Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Number i)
  interp (Couple i1 i2) = (Pair i1 i2)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
  plus (Number a) (Pair i1 i2) = (Pair (a + i1) (a + i2))
  plus (Pair i1 i2) (Number a) = (Pair (a + i1) (a + i2))
#+END_SRC

But the obvious downside of this approach is that we cannot make
changes to the original interpreter without having to manually
replicate these changes in the extended interpreter.  Replicating the
changes is time-consuming and prone to error.

Furthermore, the extended interpreter code makes no distinction
between the original code and the extension code; the two are merged
together.  Thus, to be able to reason about the changes brought by the
extension, we must always compare both programs.  We cannot reason
about the extension in isolation.

These downsides are caused by the /accidental/ duplication of code for
the original interpreter.  Conceptually, we want to reuse the behavior
the orginal code provides, but not the code itself.  Copying the code
is an easy way to reuse the original behavior, but bears the costs
mentionned above: code replication and unability to reason locally.

Is there a better way?  A /differential description/ of the extension
eliminates both issues.  If we describe only the additions and changes
to the original interpreter, we minimize code duplication, and promote
local reasoning.

** Case study: Narcissus instrumentation for faceted evaluation
[[https://github.com/mozilla/narcissus][Narcissus]] is a meta-circular interpreter for JavaScript.  Tom Austin
and Cormac Flanagan implemented their Faceted Evaluation strategy in
JavaScript by instrumenting Narcissus.  This instrumentation was done
on top of the interpreter, in a “straightforward way”, that is,
without any consideration for modularity.  The best way to know what
has changed from the base interpreter is to look at [[file:narcissus-facet.diff][this diff]]
(extracted from the HEADs of https://github.com/taustin/narcissus and
https://github.com/taustin/ZaphodFacets).

After examining the diff, three patterns are readily apparent:

1. The addition of a =pc= parameter (Program Counter) at multiple
   locations.  The program counter is part of the context needed by
   the faceted evaluation strategy; it has to be present in the
   execution context of Narcissus.  This pattern is found in the
   operational semantics of faceted evaluation.

   #+BEGIN_SRC diff
     89c99
     <     function ExecutionContext(type, version) {
     ---
     >     function ExecutionContext(type, pc, version) {
     115c131
     <             var x2 = new ExecutionContext(EVAL_CODE, x.version);
     ---
     >             var x2 = new ExecutionContext(EVAL_CODE, x.pc, x.version);
     233c361,362
     <         ExecutionContext.current = new ExecutionContext(GLOBAL_CODE, Narcissus.options.version);
     ---
     >         ExecutionContext.current = new ExecutionContext(GLOBAL_CODE,
     >                 new ProgramCounter(), Narcissus.options.version);
     313c442,462
     <     function getValue(v) {
     ---
     >     function getValue(v, pc) {
     413c577
     <         var x2 = new ExecutionContext(MODULE_CODE, x.version);
     ---
     >         var x2 = new ExecutionContext(MODULE_CODE, x.pc, x.version);
     580c765
     <             v = getValue(s);
     ---
     >             v = getValue(s, pc);
     590c775
     <                 putValue(execute(r, x), a[i], r);
     ---
     >                 putValue(execute(r, x), a[i], r, x.pc);
     1323c1529
     <         var x = new ExecutionContext(GLOBAL_CODE, Narcissus.options.version);
     ---
     >         var x = new ExecutionContext(GLOBAL_CODE, new ProgramCounter(), Narcissus.options.version);
     1411c1617
     <         var x = new ExecutionContext(GLOBAL_CODE, Narcissus.options.version);
     ---
     >         var x = new ExecutionContext(GLOBAL_CODE, new ProgramCounter(), Narcissus.options.version);

   #+END_SRC

2. The addition of tests on =FacetedValue= values in evaluation
   functions.  This is consistent with the operational semantics for
   faceted evaluation: new cases are created to handle faceted
   values.  Though the translation from operational semantics to code
   is not obvious.

   #+BEGIN_SRC diff
     <     function getValue(v) {
     ---
     >     function getValue(v, pc) {
     >         if (v instanceof FacetedValue) {
     >             return derefFacetedValue(v, pc);
     >         }
     327,329c476,493
     <     function putValue(v, w, vn) {
     <         if (v instanceof Reference)
     <             return (v.base || global)[v.propertyName] = w;
     ---
     >     function putValue(v, w, vn, pc) {
     >         if (v instanceof FacetedValue) {
     >             // x is not really an execution environment, but is being used a
     >             // way of passing on data.
     >             return evaluateEachPair(v, w, function(ref, val, x) {
     >                 return putValue(ref, val, x.vn, x.pc);
     >             }, {pc: pc, vn: vn});
     >         }
     >         else if (v instanceof Reference) {
     >             var base = v.base || global;
     >             var oldVal = base[v.propertyName];
     >             var newVal = base[v.propertyName] = buildVal(pc, w, oldVal);
     >             return w;
     >         }
     512c680,689
     <             if (getValue(execute(n.condition, x)))
     ---
     >             let cond = getValue(execute(n.condition, x), pc);
     >             if (cond instanceof FacetedValue) {
     >                 evaluateEach(cond, function(v, x) {
     >                     if (v)
     >                         execute(n.thenPart, x);
     >                     else if (n.elsePart)
     >                         execute(n.elsePart, x);
     >                 }, x);
     >             }
     >             else if (cond)

   #+END_SRC

3. The addition of =evaluateEach= in multiple locations.  This is a
   convenience function, to avoid repeating the needed pattern of
   recursively evaluating faceted values.

   #+BEGIN_SRC diff
     562,568c740,754
     <             while (!n.condition || getValue(execute(n.condition, x))) {
     ---
     >             let whileCond = !n.condition || getValue(execute(n.condition, x), pc);
     >             evaluateEach(whileCond, function(c,x) {
     658,664c850,859
     <             t = toObject(getValue(r), r, n.object);
     <             x.scope = {object: t, parent: x.scope};
     <             try {
     <                 execute(n.body, x);
     <             } finally {
     <                 x.scope = x.scope.parent;
     <             }
     ---
     >             t = getValue(r,pc);
     >             evaluateEach(t, function(t,x) {
     >                 let o = toObject(t, r, n.object);
     >                 x.scope = {object: o, parent: x.scope};
     >                 try {
     >                     execute(n.body, x);
     >                 } finally {
     >                     x.scope = x.scope.parent;
     >                 }
     >             }, x);
     735,736c943,947
     <             v = getValue(execute(c[0], x)) ? getValue(execute(c[1], x))
     <                                            : getValue(execute(c[2], x));
     ---
     >             t = getValue(execute(c[0], x), pc);
     >             v = evaluateEach(t, function(t,x) {
     >                 return t ? getValue(execute(c[1], x), x.pc)
     >                          : getValue(execute(c[2], x), x.pc);
     >             }, x);
     741c952,963
     <             v = getValue(execute(c[0], x)) || getValue(execute(c[1], x));
     ---
     >             v = getValue(execute(c[0], x), pc);
     >             if (v instanceof FacetedValue) {
     >                 let v2Thunk = function(pc) {
     >                     return getValue(execute(c[1],x), pc);
     >                 };
     >                 v = evaluateEach(v, function(v1, x) {
     >                     return v1 || v2Thunk(x.pc);
     >                 }, x);
     >             }
     >             else if (!v) {
     >                 v = getValue(execute(c[1], x), x.pc);
     >             }
     856c1010,1017
     <             v = getValue(execute(c[0], x)) % getValue(execute(c[1], x));
     ---
     >             t = getValue(execute(c[0], x), pc);
     >             u = getValue(execute(c[1], x), pc);
     >             v = evaluateEachPair(t, u, function(t, u, pc) {
     >                 if (isObject(u) && typeof u.__hasInstance__ === "function")
     >                     return u.__hasInstance__(t);
     >                 else
     >                     return t instanceof u;
     >             }, x);
     861c1022,1024
     <             v = !(t instanceof Reference) || delete t.base[t.propertyName];
     ---
     >             v = evaluateEach(t, function(t,x) {
     >                 return !(t instanceof Reference) || delete t.base[t.propertyName];
     >             }, x);

   #+END_SRC

   #+BEGIN_SRC js
     function evaluateEach(v, f, x) {
       let pc = x.pc;
       if (!(v instanceof FacetedValue)) {
         return f(v, x);
       }

       if (pc.contains(v.label)) {
         return evaluateEach(v.authorized, f, x);
       }
       else if (pc.contains(v.label.reverse())) {
         return evaluateEach(v.unauthorized, f, x);
       }
       else {
         let va, vu;
         try {
           x.pc = pc.join(v.label);
           va = evaluateEach(v.authorized, f, x);
           x.pc = pc.join(v.label.reverse());
           vu = evaluateEach(v.unauthorized, f, x);
           x.pc = pc;
         }
         catch (e) {
           // Terminate program to avoid leaking data through exceptions
           //throw END_SIGNAL;
           throw e;
         }
         return new FacetedValue(v.label, va, vu);
       }
     }
   #+END_SRC

From this preliminary examination, we gather that:
1. The Narcissus instrumentation share some similarity with the
   operational semantics for faceted evaluation.  Although it is
   difficult to tell for certain if the instrumentation is conforming
   without a specification, a proof of the translation to JavaScript,
   or even a test suite.
2. Without looking at the diff, looking only at the instrumented code,
   only a careful reading and understanding of the code would reveal
   the pieces needed by instrumentation, and the pieces needed by the
   standard interpreter.  Concerns from instrumentation and from the
   interpreter are all tangled together.

Is there a better way to write this instrumentation on top of the
interpreter?  A way that would clearly and cleanly separate the
instrumentation code from the interpreter; a way that would minimize
coupling between the instrumentation and the interpreter; a modular
way?

Before attempting to answer this tantalizing question, we should try
to reduce the noise from this Narcissus example, and unearth the core
issues.  JavaScript is a large language, and Narcissus, while
reasonable, is not a minimal example of an interpreter.

** Finding a core example
Looking at the operational semantics for faceted evaluation, we can
see the patterns mentioned previously (=pc= parameter, new cases for
=FacetedValues=).  They are based on a lambda calculus variant, with
references and a “Bottom” value.  Let’s try to write an interpreter
for this lambda calculus without anticipating the later
instrumentation.

We’ll drop the read/write rules since they only add noise to this
example.  We’ll also leave out error handling.

[[file:js/lab/lamfa-es6-standard.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function interpretNode(σ, θ, node) {
    return rules[node.type](σ, θ, node);
  }

  let ↆ = interpretNode;

  let bottom = {type: 'bottom'};

  function closure(x, e, θ) { return {type: 'closure', x, e, θ}; }
  function address(a) { return {type: 'address', a}; }

  function eval_apply(σ, v1, v2) {
    return application_rules[v1.type](σ, v1, v2);
  }

  let application_rules = {
    bottom(σ) {
      return [σ, bottom];
    },

    closure(σ, {x, e, θ}, v) {
      let θ1 = Object.create(θ);
      θ1[x] = v;
      return ↆ(σ, θ1, e);
    },
  };

  function eval_deref(σ, v) {
    return deref_rules[v.type](σ, v);
  }

  let deref_rules = {
    bottom() {
      return bottom;
    },

    address(σ, {a}) {
      return σ[a];
    },
  };

  function eval_assign(σ, v1, v2) {
    return assign_rules[v1.type](σ, v1, v2);
  }

  let assign_rules = {
    bottom(σ) {
      return σ;
    },

    address(σ, {a}, v) {
      let σ2 = Object.create(σ);
      σ2[a] = v;
      return σ2;
    },
  };

  let rules = {
    c(σ, θ, {e}) {
      return [ σ, e ];
    },

    v(σ, θ, {e}) {
      return [ σ, θ[e] ];
    },

    fun(σ, θ, {x, e}) {
      return [ σ, closure(x, e, θ) ];
    },

    app(σ, θ, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, e1);
      let [σ2, v2] = ↆ(σ1, θ, e2);
      return eval_apply(σ2, v1, v2);
    },

    ref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = v;
      return [ σ2, address(a) ];
    },

    deref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      return [ σ1, eval_deref(σ1, v) ];
    },

    assign(σ, θ, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, e1);
      let [σ2, v2] = ↆ(σ1, θ, e2);
      return [ eval_assign(σ2, v1, v2), v2 ];
    },
  };

  function interpretProgram(AST, env = {}, store = {}) {
    return interpretNode(env, store, AST);
  }

  // Test
  function app(e1, e2) { return {type: 'app', e1, e2}; }
  function fun(x, e) { return {type: 'fun', x, e}; }
  function ref(e) { return {type: 'ref', e}; }
  function deref(e) { return {type: 'deref', e}; }
  function c(e) { return {type: 'c', e}; }
  function v(e) { return {type: 'v', e}; }

  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42)))
  );
#+END_SRC

We used destructuring from ES6 and Unicode identifiers to approximate
the appearance of the big-step semantics.  To effect the operational
rules, we use an ad-hoc pattern matching.  Each AST node is an object
with a =type= field, and the =interpNode= function dispatches to the
function in the =rules= object corresponding to the value of this type
field.  The same pattern matching mechanism is used to distinguish
between an address, a closure or a bottom value.

We can easily instrument this base interpreter by following the
operational semantics from Austin and Flanagan.

[[file:js/lab/lamfa-es6-facets.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function interpretNode(σ, θ, pc, node) {
    return rules[node.type](σ, θ, pc, node);
  }

  let ↆ = interpretNode;

  let bottom = {type: 'bottom'};

  function mk_facet(pc, v1, v2) {
    if (pc.size === 0)
      return v1;

    let [k, ...rest] = pc;
    rest = new Set(rest);

    if (k > 0)
      return facet(k, mk_facet(rest, v1, v2), v2);
    else
      return facet(k, v2, mk_facet(rest, v1, v2));
  }

  function facet(k, vh, vl) { return {type: 'facet', k, vh, vl}; }
  function closure(x, e, θ) { return {type: 'closure', x, e, θ}; }
  function address(a) { return {type: 'address', a}; }

  function eval_apply(σ, pc, v1, v2) {
    return application_rules[v1.type](σ, pc, v1, v2);
  }

  let application_rules = {
    bottom(σ) {
      return [σ, bottom];
    },

    closure(σ, pc, {x, e, θ}, v) {
      let θ1 = Object.create(θ);
      θ1[x] = v;
      return ↆ(σ, θ1, pc, e);
    },

    facet(σ, pc, {k, vh, vl}, v2) {
      if (pc.has(k)) {
        return eval_apply(σ, pc, vh, v2);
      }

      else if (pc.has(-k)) {
        return eval_apply(σ, pc, vl, v2);
      }

      else {
        let [σ1, vh1] = eval_apply(σ, pc.union(k), vh, v2);
        let [σ2, vl1] = eval_apply(σ1, pc.union(-k), vl, v2);
        return [ σ2, mk_facet(k, vh1, vl1) ];
      }
    },
  };

  function eval_deref(σ, v, pc) {
    return deref_rules[v.type](σ, v, pc);
  }

  let deref_rules = {
    bottom() {
      return bottom;
    },

    address(σ, {a}, pc) {
      return σ[a];
    },

    facet(σ, {k, vh, vl}, pc) {
      if (pc.has(k))
        return eval_deref(σ, vh, pc);
      else if (pc.has(-k))
        return eval_deref(σ, vl, pc);
      else
        return mk_facet(k, eval_deref(σ, vh, pc), eval_deref(σ, vl, pc));
    },
  };

  function eval_assign(σ, pc, v1, v2) {
    return assign_rules[v1.type](σ, pc, v1, v2);
  }

  let assign_rules = {
    bottom(σ) {
      return σ;
    },

    address(σ, pc, {a}, v) {
      let σ2 = Object.create(σ);
      σ2[a] = mk_facet(pc, v, σ[a]);
      return σ2;
    },

    facet(σ, pc, {k, vh, vl}, v) {
      let σ1 = eval_assign(σ, pc.union(k), vh, v);
      return eval_assign(σ1, pc.union(-k), vl, v);
    },
  };

  let rules = {
    c(σ, θ, pc, {e}) {
      return [ σ, e ];
    },

    v(σ, θ, pc, {e}) {
      return [ σ, θ[e] ];
    },

    fun(σ, θ, pc, {x, e}) {
      return [ σ, closure(x, e, θ) ];
    },

    app(σ, θ, pc, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, pc, e1);
      let [σ2, v2] = ↆ(σ1, θ, pc, e2);
      return eval_apply(σ2, pc, v1, v2);
    },

    ref(σ, θ, pc, {e}) {
      let [σ1, v] = ↆ(σ, θ, pc, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    },

    deref(σ, θ, pc, {e}) {
      let [σ1, v] = ↆ(σ, θ, pc, e);
      return [ σ1, eval_deref(σ1, v, pc) ];
    },

    assign(σ, θ, pc, {e1, e2}) {
      let [σ1, v1] = ↆ(σ, θ, pc, e1);
      let [σ2, v2] = ↆ(σ1, θ, pc, e2);
      return [ eval_assign(σ2, pc, v1, v2), v2 ];
    },
  };

  function interpretProgram(AST, env = {}, store = {}, pc = []) {
    let pc = new Set(pc);
    return interpretNode(env, store, pc, AST);
  }

  // Test
  function app(e1, e2) { return {type: 'app', e1, e2}; }
  function fun(x, e) { return {type: 'fun', x, e}; }
  function ref(e) { return {type: 'ref', e}; }
  function deref(e) { return {type: 'deref', e}; }
  function c(e) { return {type: 'c', e}; }
  function v(e) { return {type: 'v', e}; }

  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42))),
    {}, {}, [1]
  );
#+END_SRC

The interesting story is told by looking at the differences between
these two versions.  We see patterns 1 and 2 reappear from our
analysis of the Narcissus instrumentation.  The =pc= parameter must be
passed around in nearly every function, and new cases must be added to
handle facet values.

#+BEGIN_SRC sh :results pp
  diff js/lab/lamfa-es6-standard.js js/lab/lamfa-es6-facets.js; exit 0
#+END_SRC

#+begin_src diff
7,8c7,8
< function interpretNode(σ, θ, node) {
<   return rules[node.type](σ, θ, node);
---
> function interpretNode(σ, θ, pc, node) {
>   return rules[node.type](σ, θ, pc, node);
14a15,28
> function mk_facet(pc, v1, v2) {
>   if (pc.size === 0)
>     return v1;
>
>   let [k, ...rest] = pc;
>   rest = new Set(rest);
>
>   if (k > 0)
>     return facet(k, mk_facet(rest, v1, v2), v2);
>   else
>     return facet(k, v2, mk_facet(rest, v1, v2));
> }
>
> function facet(k, vh, vl) { return {type: 'facet', k, vh, vl}; }
18,19c32,33
< function eval_apply(σ, v1, v2) {
<   return application_rules[v1.type](σ, v1, v2);
---
> function eval_apply(σ, pc, v1, v2) {
>   return application_rules[v1.type](σ, pc, v1, v2);
27c41
<   closure(σ, {x, e, θ}, v) {
---
>   closure(σ, pc, {x, e, θ}, v) {
30c44,60
<     return ↆ(σ, θ1, e);
---
>     return ↆ(σ, θ1, pc, e);
>   },
>
>   facet(σ, pc, {k, vh, vl}, v2) {
>     if (pc.has(k)) {
>       return eval_apply(σ, pc, vh, v2);
>     }
>
>     else if (pc.has(-k)) {
>       return eval_apply(σ, pc, vl, v2);
>     }
>
>     else {
>       let [σ1, vh1] = eval_apply(σ, pc.union(k), vh, v2);
>       let [σ2, vl1] = eval_apply(σ1, pc.union(-k), vl, v2);
>       return [ σ2, mk_facet(k, vh1, vl1) ];
>     }
34,35c64,65
< function eval_deref(σ, v) {
<   return deref_rules[v.type](σ, v);
---
> function eval_deref(σ, v, pc) {
>   return deref_rules[v.type](σ, v, pc);
43c73
<   address(σ, {a}) {
---
>   address(σ, {a}, pc) {
45a76,84
>
>   facet(σ, {k, vh, vl}, pc) {
>     if (pc.has(k))
>       return eval_deref(σ, vh, pc);
>     else if (pc.has(-k))
>       return eval_deref(σ, vl, pc);
>     else
>       return mk_facet(k, eval_deref(σ, vh, pc), eval_deref(σ, vl, pc));
>   },
48,49c87,88
< function eval_assign(σ, v1, v2) {
<   return assign_rules[v1.type](σ, v1, v2);
---
> function eval_assign(σ, pc, v1, v2) {
>   return assign_rules[v1.type](σ, pc, v1, v2);
57c96
<   address(σ, {a}, v) {
---
>   address(σ, pc, {a}, v) {
59c98
<     σ2[a] = v;
---
>     σ2[a] = mk_facet(pc, v, σ[a]);
61a101,105
>
>   facet(σ, pc, {k, vh, vl}, v) {
>     let σ1 = eval_assign(σ, pc.union(k), vh, v);
>     return eval_assign(σ1, pc.union(-k), vl, v);
>   },
65c109
<   c(σ, θ, {e}) {
---
>   c(σ, θ, pc, {e}) {
69c113
<   v(σ, θ, {e}) {
---
>   v(σ, θ, pc, {e}) {
73c117
<   fun(σ, θ, {x, e}) {
---
>   fun(σ, θ, pc, {x, e}) {
77,80c121,124
<   app(σ, θ, {e1, e2}) {
<     let [σ1, v1] = ↆ(σ, θ, e1);
<     let [σ2, v2] = ↆ(σ1, θ, e2);
<     return eval_apply(σ2, v1, v2);
---
>   app(σ, θ, pc, {e1, e2}) {
>     let [σ1, v1] = ↆ(σ, θ, pc, e1);
>     let [σ2, v2] = ↆ(σ1, θ, pc, e2);
>     return eval_apply(σ2, pc, v1, v2);
83,84c127,128
<   ref(σ, θ, {e}) {
<     let [σ1, v] = ↆ(σ, θ, e);
---
>   ref(σ, θ, pc, {e}) {
>     let [σ1, v] = ↆ(σ, θ, pc, e);
87c131
<     σ2[a] = v;
---
>     σ2[a] = mk_facet(pc, v, bottom);
91,93c135,137
<   deref(σ, θ, {e}) {
<     let [σ1, v] = ↆ(σ, θ, e);
<     return [ σ1, eval_deref(σ1, v) ];
---
>   deref(σ, θ, pc, {e}) {
>     let [σ1, v] = ↆ(σ, θ, pc, e);
>     return [ σ1, eval_deref(σ1, v, pc) ];
96,99c140,143
<   assign(σ, θ, {e1, e2}) {
<     let [σ1, v1] = ↆ(σ, θ, e1);
<     let [σ2, v2] = ↆ(σ1, θ, e2);
<     return [ eval_assign(σ2, v1, v2), v2 ];
---
>   assign(σ, θ, pc, {e1, e2}) {
>     let [σ1, v1] = ↆ(σ, θ, pc, e1);
>     let [σ2, v2] = ↆ(σ1, θ, pc, e2);
>     return [ eval_assign(σ2, pc, v1, v2), v2 ];
103,104c147,149
< function interpretProgram(AST, env = {}, store = {}) {
<   return interpretNode(env, store, AST);
---
> function interpretProgram(AST, env = {}, store = {}, pc = []) {
>   let pc = new Set(pc);
>   return interpretNode(env, store, pc, AST);
117c162,163
<       ref(c(42)))
---
>       ref(c(42))),
>   {}, {}, [1]
#+end_src

Our ad-hoc pattern matching is perhaps not the most straightforward
way to write such an interpreter in JavaScript.  Another, maybe more
familiar way is to use object-oriented dispatching [fn:: And indeed,
this form is quite similar to the Interpreter pattern from Gamma et
al.].

[[file:js/lab/lamfa-oo-standard.js]]
#+BEGIN_SRC js
  let bottom = {
    eval_apply(σ) {
      return [ σ, bottom ];
    },

    eval_deref() {
      return bottom;
    },

    eval_assign(σ) {
      return σ;
    }
  };

  function address(a) {
    return {
      eval_deref(σ) {
        return σ[a];
      },

      eval_assign(σ, v) {
        let σ2 = Object.create(σ);
        σ2[a] = v;
        return σ2;
      }
    };
  }

  function closure(x, e, θ) {
    return {
      eval_apply(σ, v) {
        let θ1 = Object.create(θ);
        θ1[x] = v;
        return e.eval(σ, θ1);
      }
    };
  }

  function c(e) {
    return {
      eval(σ, θ) {
        return [ σ, e ];
      }
    };
  }

  function v(e) {
    return {
      eval(σ, θ) {
        return [ σ, θ[e] ];
      }
    };
  }

  function fun(x, e) {
    return {
      eval(σ, θ) {
        return [ σ, closure(x, e, θ) ];
      }
    };
  }

  function app(e1, e2) {
    return {
      eval(σ, θ) {
        let [σ1, v1] = e1.eval(σ, θ);
        let [σ2, v2] = e2.eval(σ1, θ);
        return v1.eval_apply(σ2, v2);
      }
    };
  }

  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = v;
        return [ σ2, address(a) ];
      }
    };
  }

  function deref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        return [ σ1, v.eval_deref(σ1, v) ];
      }
    };
  }

  function assign(e1, e2) {
    return {
      eval(σ, θ) {
        let [σ1, v1] = e1.eval(σ, θ);
        let [σ2, v2] = e2.eval(σ1, θ);
        return [ v1.eval_assign(σ2, v2), v2 ];
      }
    };
  }

  function interpretProgram(AST, env = {}, store = {}) {
    return AST.eval(env, store);
  }

  // Test
  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42)))
  );
#+END_SRC

[[file:js/lab/lamfa-oo-facets.js]]
#+BEGIN_SRC js
  Set.prototype.union = function(elem) {
    let n = new Set(this);
    n.add(elem);
    return n;
  }

  function mk_facet(pc, v1, v2) {
    if (pc.size === 0)
      return v1;

    let [k, ...rest] = pc;
    rest = new Set(rest);

    if (k > 0)
      return facet(k, mk_facet(rest, v1, v2), v2);
    else
      return facet(k, v2, mk_facet(rest, v1, v2));
  }

  function facet(k, vh, vl) {
    return {
      eval_apply(σ, pc, v2) {
        if (pc.has(k)) {
          return vh.eval_apply(σ, pc, v2);
        }

        else if (pc.has(-k)) {
          return vl.eval_apply(σ, pc, v2);
        }

        else {
          let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
          let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
          return [ σ2, mk_facet(k, vh1, vl1) ];
        }
      },

      eval_deref(σ, pc) {
        if (pc.has(k))
          return vh.eval_deref(σ, pc);
        else if (pc.has(-k))
          return vl.eval_deref(σ, pc);
        else
          return mk_facet(k, vh.eval_deref(σ, pc), vl.eval_deref(σ, pc));
      },

      eval_assign(σ, pc, v) {
        let σ1 = vh.eval_assign(σ, pc.union(k), v);
        return vl.eval_assign(σ1, pc.union(-k), v);
      }
    };
  }

  let bottom = {
    eval_apply(σ) {
      return [ σ, bottom ];
    },

    eval_deref() {
      return bottom;
    },

    eval_assign(σ) {
      return σ;
    }
  };

  function address(a) {
    return {
      eval_deref(σ) {
        return σ[a];
      },

      eval_assign(σ, v) {
        let σ2 = Object.create(σ);
        σ2[a] = v;
        return σ2;
      }
    };
  }

  function closure(x, e, θ) {
    return {
      eval_apply(σ, pc, v) {
        let θ1 = Object.create(θ);
        θ1[x] = v;
        return e.eval(σ, θ1, pc);
      }
    };
  }

  function c(e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, e ];
      }
    };
  }

  function v(e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, θ[e] ];
      }
    };
  }

  function fun(x, e) {
    return {
      eval(σ, θ, pc) {
        return [ σ, closure(x, e, θ) ];
      }
    };
  }

  function app(e1, e2) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v1] = e1.eval(σ, θ, pc);
        let [σ2, v2] = e2.eval(σ1, θ, pc);
        return v1.eval_apply(σ2, pc, v2);
      }
    };
  }

  function ref(e) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v] = e.eval(σ, θ, pc);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = mk_facet(pc, v, bottom);
        return [ σ2, address(a) ];
      }
    };
  }

  function deref(e) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v] = e.eval(σ, θ, pc);
        return [ σ1, v.eval_deref(σ1, pc, v) ];
      }
    };
  }

  function assign(e1, e2) {
    return {
      eval(σ, θ, pc) {
        let [σ1, v1] = e1.eval(σ, θ, pc);
        let [σ2, v2] = e2.eval(σ1, θ, pc);
        return [ v1.eval_assign(σ2, pc, v2), v2 ];
      }
    };
  }

  function interpretProgram(AST, env = {}, store = {}, pc = []) {
    let pc = new Set(pc);
    return AST.eval(env, store, pc);
  }

  // Test
  interpretProgram(
    app(fun('x', deref(v('x'))),
        ref(c(42))),
    {}, {}, [1]
  );

#+END_SRC

But here again, the pattern appears when looking at the diff.

#+BEGIN_SRC sh :results pp
  diff js/lab/lamfa-oo-standard.js js/lab/lamfa-oo-facets.js; exit 0
#+END_SRC

#+begin_src diff
0a1,53
> Set.prototype.union = function(elem) {
>   let n = new Set(this);
>   n.add(elem);
>   return n;
> }
>
> function mk_facet(pc, v1, v2) {
>   if (pc.size === 0)
>     return v1;
>
>   let [k, ...rest] = pc;
>   rest = new Set(rest);
>
>   if (k > 0)
>     return facet(k, mk_facet(rest, v1, v2), v2);
>   else
>     return facet(k, v2, mk_facet(rest, v1, v2));
> }
>
> function facet(k, vh, vl) {
>   return {
>     eval_apply(σ, pc, v2) {
>       if (pc.has(k)) {
>         return vh.eval_apply(σ, pc, v2);
>       }
>
>       else if (pc.has(-k)) {
>         return vl.eval_apply(σ, pc, v2);
>       }
>
>       else {
>         let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
>         let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
>         return [ σ2, mk_facet(k, vh1, vl1) ];
>       }
>     },
>
>     eval_deref(σ, pc) {
>       if (pc.has(k))
>         return vh.eval_deref(σ, pc);
>       else if (pc.has(-k))
>         return vl.eval_deref(σ, pc);
>       else
>         return mk_facet(k, vh.eval_deref(σ, pc), vl.eval_deref(σ, pc));
>     },
>
>     eval_assign(σ, pc, v) {
>       let σ1 = vh.eval_assign(σ, pc.union(k), v);
>       return vl.eval_assign(σ1, pc.union(-k), v);
>     }
>   };
> }
>
31c84
<     eval_apply(σ, v) {
---
>     eval_apply(σ, pc, v) {
34c87
<       return e.eval(σ, θ1);
---
>       return e.eval(σ, θ1, pc);
41c94
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
49c102
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
57c110
<     eval(σ, θ) {
---
>     eval(σ, θ, pc) {
65,68c118,121
<     eval(σ, θ) {
<       let [σ1, v1] = e1.eval(σ, θ);
<       let [σ2, v2] = e2.eval(σ1, θ);
<       return v1.eval_apply(σ2, v2);
---
>     eval(σ, θ, pc) {
>       let [σ1, v1] = e1.eval(σ, θ, pc);
>       let [σ2, v2] = e2.eval(σ1, θ, pc);
>       return v1.eval_apply(σ2, pc, v2);
75,76c128,129
<     eval(σ, θ) {
<       let [σ1, v] = e.eval(σ, θ);
---
>     eval(σ, θ, pc) {
>       let [σ1, v] = e.eval(σ, θ, pc);
79c132
<       σ2[a] = v;
---
>       σ2[a] = mk_facet(pc, v, bottom);
87,89c140,142
<     eval(σ, θ) {
<       let [σ1, v] = e.eval(σ, θ);
<       return [ σ1, v.eval_deref(σ1, v) ];
---
>     eval(σ, θ, pc) {
>       let [σ1, v] = e.eval(σ, θ, pc);
>       return [ σ1, v.eval_deref(σ1, pc, v) ];
96,99c149,152
<     eval(σ, θ) {
<       let [σ1, v1] = e1.eval(σ, θ);
<       let [σ2, v2] = e2.eval(σ1, θ);
<       return [ v1.eval_assign(σ2, v2), v2 ];
---
>     eval(σ, θ, pc) {
>       let [σ1, v1] = e1.eval(σ, θ, pc);
>       let [σ2, v2] = e2.eval(σ1, θ, pc);
>       return [ v1.eval_assign(σ2, pc, v2), v2 ];
104,105c157,159
< function interpretProgram(AST, env = {}, store = {}) {
<   return AST.eval(env, store);
---
> function interpretProgram(AST, env = {}, store = {}, pc = []) {
>   let pc = new Set(pc);
>   return AST.eval(env, store, pc);
111c165,166
<       ref(c(42)))
---
>       ref(c(42))),
>   {}, {}, [1]
#+end_src

Hence, these patterns appear regardless of the instrumented language
(lambda calculus or JavaScript), and regardless of the language
features used by the implementation (pattern matching or dynamic
dispatch).  Therefore, the problem of finding a way to describe the
instrumentation as a module and minimizing coupling is not specific to
Narcissus or JavaScript interpreters.

We will now exhibit variations in writing the instrumentation as
a separate module with minimal code duplication.

** The expression problem
First, let’s list the additions brought by our extension to the
interpreter:

- we add a new constructor =Couple= to the data type =Term=
- we add a new constructor =Pair= to the data type =Value=
- we add a new case =Couple= to the function =interp=
- we add two new (symmetric) cases =Pair= to the function =plus=

Now, what would a differential description of the extension to our
interpreter look like?

#+BEGIN_SRC haskell
  extend data Term = Couple Int Int
  extend data Value = Pair Int Int

  extend interp (Couple i1 i2) = (Pair i1 i2)
  extend plus (Number a) (Pair i1 i2) = (Pair (a + i1) (a + i2))
  extend plus (Pair i1 i2) (Number a) = (Pair (a + i1) (a + i2))
#+END_SRC

These are all the additions brought by the extension to the original
interpreter.  The new =extend= keyword allows us to:

1. Extend data types with new constructors.
2. Extend function definitions with new cases.

How to extend both data types and functions in a program, without
sacrificing modularity, is a problem known as the /expression problem/
[Wadler].  This (imaginary) =extend= keyword is a solution; there are
real solutions [[[http://www.andres-loeh.de/OpenDatatypes.pdf][Open Data Types and Open Functions]]] (Instead of an
=extend= keyword, they provide an =open= keyword to prefix to initial
declarations of data types and functions.  One could also require both
extended and original codes to include keywords.)

** The expression problem, with a twist
The expression problem is only concerned with /adding/ data types and
functions, but when we instrument an interpreter, we will often want
to /modify/ its behavior rather than just extend it.

When we take modification into account, what does a differential
description look like?

First, let’s go back to the original interpreter, and modify its
behavior.

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int
  data Value = Number Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Number i)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
#+END_SRC

Say we need to change the value returned by the interpretation of the
term ‘Constant’.  We want to return a ‘Pair’ value instead of a single
number, the second value being a default zero.  We would write the
full version, with replication as:

#+BEGIN_SRC haskell
  data Term = Plus Term Term | Constant Int
  data Value = Number Int | Pair Int Int

  interp :: Term -> Value
  interp (Plus t1 t2) = plus (interp t1) (interp t2)
  interp (Constant i) = (Pair i 0)

  plus :: Value -> Value -> Value
  plus (Number a) (Number b) = (Number (a + b))
#+END_SRC

What would be the differential description of this change?

#+BEGIN_SRC haskell
  extend data Value = Pair Int Int

  modify interp (Constant i) = (Pair i 0)
#+END_SRC

The keyword =modify= replaces the definition for the targeted case of
=interp=.

[What if we want to combine multiple modifications to the same case
function? A: You’d have to have a clue about the precedence order to
make sense of the result, but these problems are shared by other AOP
applications.]

** The modular instrumentation problem
We have an interpreter I for a language L, and the source code for I.
We want to instrument the interpreter I, by extending and modifying
its behavior.  Namely, the instrumentation can:
- add terms to the base language
- add values to the base language
- add new operations
- alter the behavior of existing operations, or even suppress them
  entirely.

We constrain the instrumentation by imposing the following
restrictions:

- The instrumented interpreter I’ must still be able to execute
  programs written in the language L.  The instrumentation cannot
  remove or modify existing terms of the language.
- The instrumentation must modify only a part of the original
  interpreter operations.  Otherwise, the instrumented interpreter
  may end up with semantics so different from the original interpreter
  that it does not qualify as “instrumentation” anymore; it might as
  well be another interpreter in its own right.

The implementation of this instrumentation will give a new interpreter
I’.  Ideally, this implementation should be as modular as possible; it
should:
- promote isolated reasoning,
- minimize code replication and accidental complexity.  We should be
  able to map the differential description of the instrumentation and
  the code for its implementation.

The /modular instrumentation problem/ is then: how to implement the
instrumentation with the above constraints of modularity?

Note that the changes may bring only additional side effects, and
leave the original behavior unaltered.  How to recognize or enforce
“side-effects only” instrumentation is an interesting
question. [“Recognize” I don’t know how.  “Enforce” you can do with
monads, if you have a monadic interpreter.]

* Variations
** JavaScript
See [[file:js/aoping.org]].

See also [[file:lassy15.org]] for a way to build interpreters
incrementally.

*** Split OO-style instrumented interpreter into modules
Let’s try to separate the object-oriented instrumented interpreter in
two modules: one for the base interpreter, ‘base.js’, and one for the
instrumentation, ‘facets.js’.  The ‘base.js’ file should not contain
any instrumentation-specific code, and be as close as possible to the
standard interpreter.

Looking at the diff, we can put =mk_facet=, =facet= and the extension
to =Set.prototype= in a separate file right away.

**** Handling the extra PC parameter
The second, more pervasive change is the addition of the program
counter context in every call.  There are at least two solutions to
this problem:
1. Using a global variable.
2. Using a context object.

***** Using a global variable
[[file:js/lab/oo-split-global/base.js]]
[[file:js/lab/oo-split-global/facets.js]]

We can use a global variable for the ‘pc’.  This eliminates the need
to pass the ‘pc’ as a formal parameter to most functions, and to pass
it down to tail calls.  However, we then need to save the previous
value of this global pc when temporarily changing the current pc to
evaluate branches in =facet.eval_assign= and =facet.eval_apply=.

With the ‘pc’ argument:
#+BEGIN_SRC js
  let [σ1, vh1] = vh.eval_apply(σ, pc.union(k), v2);
  let [σ2, vl1] = vl.eval_apply(σ1, pc.union(-k), v2);
#+END_SRC

With a global variable ‘pc’:
#+BEGIN_SRC js
  let pc_old = pc;
  pc = pc.union(k);
  let [σ1, vh1] = vh.eval_apply(σ, v2);
  pc = pc.union(-k);
  let [σ2, vl1] = vl.eval_apply(σ1, v2);
  pc = pc_old;
#+END_SRC

This temporary rewriting of a variable is essentially emulating a
dynamic binding of the ‘pc’ variable.  There are no built-ins
constructs or syntactic sugar for dynamic binding in JavaScript, but
we can add one for this particular variable.

#+BEGIN_SRC js
  function with_pc(new_pc, thunk) {
    let old_pc = pc;
    pc = new_pc;
    let ret = thunk();
    pc = old_pc;
    return ret;
  }
#+END_SRC

Now the =facet.eval_apply= function is cleaner:

#+BEGIN_SRC js
  let [σ1, vh1] = with_pc(pc.union(k), () => vh.eval_apply(σ, v2));
  let [σ2, vl1] = with_pc(pc.union(-k), () => vl.eval_apply(σ1, v2));
  return [ σ2, mk_facet(k, vh1, vl1) ];
#+END_SRC

The ‘pc’ parameter is thus part of the closure of all facet-related
functions.  The =with_pc= construct require mutability in order to
change the current program counter referenced in the closure.
Mutability can be waived as a requirement if the language supports
dynamic scoping [Art of Interp].

***** Using a context object
[[file:js/lab/oo-split-context/base.js]]
[[file:js/lab/oo-split-context/facets.js]]

Using a global variable for just the ‘pc’ and not the store or
environment seems heterogeneous.  We can adopt the position that the
‘pc’ is an extension to the state of the interpreter, and bundle all
this state in a ‘context’ objet passed around in function calls.  Then
we profit from the dynamic nature of JavaScript objects: we can add
any property at runtime.  Rather than hiding the ‘pc’ away, it makes
the state passing explicit, and removes the need for =with_pc=.

This solution has the benefit of homogenizing the order of formal
parameters: the context object will always be the first one, and the
=eval= functions will always return a context object rather than just
the store.

Here is what the third branch of =facet.eval_apply= looks like:

#+BEGIN_SRC js
  let [C1, vh1] = vh.eval_apply(with_pc(C, C.pc.union(k)), v2);
  let [C2, vl1] = vl.eval_apply(with_pc(C1, C.pc.union(-k)), v2);
  return [ C2, mk_facet(k, vh1, vl1) ];
#+END_SRC

The evaluation of functions now explicitly deals with contexts:

#+BEGIN_SRC js
  function app(e1, e2) {
    return {
      eval(C) {
        let [C1, v1] = e1.eval(C);
        let [C2, v2] = e2.eval(C1);
        return v1.eval_apply(C2, v2);
      }
    };
  }
#+END_SRC

The signatures are simpler, though we lose a bit in legibility, as =C=
is opaque: we do not see what the context made is of by looking at the
formal parameters.  Only looking at the entry point
(=interpretProgram=) reveals its contents.

The context object bundles all the state needed by the interpreter,
and its extensions.  Adding another piece of state is modular since
the base interpreter already passes down this context object.

This solution does not require mutability, but it benefits from
dynamic typing.  In the case of the base interpreter, the context
object has type (Store * Environment), while in the faceted
interpreter, it has type (Store * Environment * PC).

Note that we could combine both methods, and avoid passing the context
object explicitly in all functions by using a global variable.  In
that case, both mutability and dynamic typing are leveraged.

**** Handling the change in the reference rule
[[file:js/lab/oo-split-global/base.js]]
[[file:js/lab/oo-split-global/facets.js]]

With the extra ‘pc’ out of the way, the only change remaining is the
=mk_facet= call in the reference rule.  This is how runtime faceted
values are effectively created.

#+BEGIN_SRC diff
79c79
<       σ2[a] = mk_facet(pc, v, bottom);
---
>       σ2[a] = v;
#+END_SRC

We can solve this by creating two different versions of the =ref=
function, one with the standard behavior, and one suited for the
instrumentation.

#+BEGIN_SRC js
  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = v;
        return [ σ2, address(a) ];
      }
    };
  }
#+END_SRC

#+BEGIN_SRC js
  function ref(e) {
    return {
      eval(σ, θ) {
        let [σ1, v] = e.eval(σ, θ);
        let a = Object.keys(σ1).length;
        let σ2 = Object.create(σ1);
        σ2[a] = mk_facet(pc, v, bottom); // this line changes
        return [ σ2, address(a) ];
      }
    };
  }
#+END_SRC

But how do we differentiate which version of =ref= to use when calling
=interpretProgram=?  We need namespaces.  Using objects as namespaces,
both versions of =ref= can coexist.

#+BEGIN_SRC js
  this.base = {
    interpretProgram,
    c, v, fun, app, ref, deref, assign,
    _innards: { bottom, address, closure}
  };
#+END_SRC

The base interpreter exports both public interface and a “restricted”
one intended for extension purposes.

#+BEGIN_SRC js
  this.facets = {
    __proto__: this.base,
    interpretProgram,
    ref
  };
#+END_SRC

The instrumented interpreter exports the same interface as the base,
only overriding definitions for =ref= and =interpretProgram=.

Both interpreters can then be tested independently.

#+BEGIN_SRC js
  with (base) {
    console.log('std', interpretProgram(
      app(fun('x', deref(v('x'))),
          ref(c(42))),
      {}, {}
    ));
  }

  with (facets) {
    console.log('facets', interpretProgram(
      app(fun('x', deref(v('x'))),
          ref(c(42))),
      {}, {}, [1]
    ));
  }
#+END_SRC

Finally, both base interpreters (with a global ‘pc’, and with a
context object) are free of instrumentation-specific concerns.  In
fact, the base interpreter with the global ‘pc’ is /identical/ to the
base object-oriented interpreter.

**** Summary
We split the object-oriented interpreter into two modules:
1. The base interpreter which contains no concerns pertaining to
   faceted evaluation.
2. The facet interpreter which extends the context of the base
   interpreter, either by using a (delimited) global, or by extending
   the context object.  The facet interpreter then selects and
   overrides the entry point (=interpretProgram=) and the reference
   rule (=ref=).

We used two different mechanisms to handle the extra ‘pc’ parameter:
1. A ‘pc’ variable in the closure of all facet-related functions.
   This solution works if the language provides mutable variables or
   dynamic scoping.
2. Modifying the base interpreter to use a context object for the
   store and environment.  We then leveraged the ability to add
   properties to existing objects and included the program counter in
   this context.  Mutability is not required, but dynamic typing
   allows us to use a context object with two incompatibles types.

Overriding the =ref= function required only delegation using the
prototype chain.

*** Split pattern-matching instrumented interpreter into modules
[[file:js/lab/es6-split-global/base.js]]
[[file:js/lab/es6-split-global/facets.js]]

We can handle the extra ‘pc’ parameter in the same ways we did with
the object-oriented interpreter.

However, when we try to extend the =ref= rule, two things are
different.

**** The facet-specific rules are scattered
The =eval_apply=, =eval_deref= and =eval_assign= code for facet values
is split across the three =application_rules=, =deref_rules= and
=assign_rules= objects.  In the object-oriented approach, they were
regrouped under the same =facet= object.

#+BEGIN_SRC js
  function facet(k, vh, vl) {
    return {
      eval_apply(σ, v2) {...},
      eval_deref(σ) {...},
      eval_assign(σ, v) {...}
    };
  }
#+END_SRC

#+BEGIN_SRC js
  base.application_rules.facet = (σ, {k, vh, vl}, v2) => {...};
  base.deref_rules.facet = (σ, {k, vh, vl}) => {...};
  base.assign_rules.facet = (σ, {k, vh, vl}, v) => {...};
#+END_SRC

This is a manifestation of the expression problem, or the “tyranny of
the primary decomposition”: in the object-oriented approach, adding a
term is just adding an object with all its evaluation functions; while
adding evaluation functions requires modifying all the objects.  Note
that since in JavaScript objects can be extended at runtime, the
primary decomposition has a lower impact.

However, here we have to add the =facet= rules to the base objects
directly.  A more modular approach would be to create new rules
objects that extends (by prototype links) the base rules objects.  But
this is not an option here because the base rules objects are enclosed
by the evaluation functions.  If we create a =facet_application_rules=
object like so:

#+BEGIN_SRC js
  facet_application_rules = {
    __proto__: base.application_rules,
    facet(σ, {k, vh, vl}, v2) => {...}
  };
#+END_SRC

Then we have to redefine the =eval_apply= function to call this new
object instead of the =base.application_rules=.

#+BEGIN_SRC js
  function eval_apply(σ, v1, v2) {
    return facet_application_rules[v1.type](σ, v1, v2);
  }
#+END_SRC

But =eval_apply= is in the closure of the =rules.app= function, so we
have to redefine it as well just to be able to update its closure.

#+BEGIN_SRC js
  app(σ, θ, {e1, e2}) {
    ...
    return eval_apply(σ2, v1, v2);
  },
#+END_SRC

And we would have to do the same for every rules object, and for every
function that refer to these rules object.  In the end, we are back to
copy-pasting the base interpreter just to update the closures of its
functions.

It is clear that the lexical scoping of the rules object is the issue.
Being able to refer to these object from a dynamic scope would resolve
it.

**** Extending the ref function
In the object-oriented approach, we redefined the =ref= function in
the facets module and delegated the other rules to the base
interpreter.  Here we cannot do so.

In the object-oriented interpreter, the =ref= function returns an
object which contains its own evaluation method.  Thus, by overriding
this =ref= function we can change the way =ref= nodes are evaluated.
In the client code, a call to =ref= is dynamically dispatched to
either the base or facet version.

In the pattern matching interpreter, the =ref= function returns an
object /without/ an evaluation function -- only bearing a type used by
=interpretNode= to dispatch to the correct evaluation function.  The
evaluation function is in the =rules= object, separated from the
object created by =ref=.  To change the evaluation of =ref= nodes, we
need to change the =ref= function inside the =rules= object, but we
also need to update the dispatching function.

The =rules= object is in the closure of =base.interpretNode=.
Extending the =rules= object with a new =ref= function would not
change the behavior of =base.interpretNode=.

#+BEGIN_SRC js
  let rules = {
    __proto__: base.rules,
    ref(σ, θ, {e}) {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    }
  };
#+END_SRC

Trying to redefine a new =interpretNode= function in the facets module
to close over this new =rules= object is not a solution.  Rules from
the base interpreter will still call the =base.interpretNode= function
which refer to the =base.rules= object.  Redefining every function
that calls =base.interpretNode= to call =facets.interpretNode= would
work, but that’s basically duplicating the base interpreter.

What does work is to ‘fluid-let’ the =rules= object inside
=facets.interpretProgram=.

#+BEGIN_SRC js
  function interpretProgram(AST, env = {}, store = {}, default_pc = []) {
    let old_ref = base._innards.rules.ref;
    base._innards.rules.ref = (σ, θ, {e}) => {
      let [σ1, v] = ↆ(σ, θ, e);
      let a = Object.keys(σ1).length;
      let σ2 = Object.create(σ1);
      σ2[a] = mk_facet(pc, v, bottom);
      return [ σ2, address(a) ];
    };
    let r = with_pc(new Set(default_pc), () =>
                    base.interpretProgram(AST, env, store));
    base._innards.rules.ref = old_ref;
    return r;
  }
#+END_SRC

Note that, again, this construct is emulating a dynamic scoping of the
=rules.ref= function.  We could define a function similar to =with_pc=
and write the following instead:

#+BEGIN_SRC js
  function interpretProgram(AST, env = {}, store = {}, default_pc = []) {
    return with_ref((σ, θ, {e}) => { ... }, () =>
                    with_pc(new Set(default_pc), () =>
                            base.interpretProgram(AST, env, store)));
  }
#+END_SRC

But at this point, having to write several ‘with’ functions becomes a
pattern we would like to abstract away once and for all.

Furthermore, this emulation is a brittle way of extending the
functionality.  First we add coupling by using the =base._innards=
interface; secondly we disallow any opportunity for combining
extensions (since we do not /extend/ but /replace/ the base
functionality).  A proper dynamic scoping of the =rules= object has
none of these downsides.

As a compromise, we can recognize that the pattern matching done for
AST nodes is trivial: the objects returned by =app=, =ref=, etc. only
have one method, =eval=.  Instead of returning objects, we can return
closures directly and eliminate the need for the =rules= object.

[[file:js/lab/closures/base.js]]
[[file:js/lab/closures/facets.js]]

#+BEGIN_SRC js
  function ref(e) {
    return (σ, θ) => { ... };
  }
#+END_SRC

This allows us to override the functionality of =ref= using a
namespace, as we did in the object-oriented approach.

**** Summary
The two solutions (global variable and context object) for dealing
with the extra ‘pc’ parameter can be applied here as well.  The same
remarks apply.

However, extending the =ref= function requires the ability to modify
values inside the closures of the evaluation function =interpretNode=.
This ability is not provided by the language, but shadowing the =ref=
function (or the rules object, or the =interpretNode= dispatcher) by
using dynamic scoping achieves the same effects.

*** Summary of JavaScript variations
It appears that, the more the interpreter rely on dynamic features,
the easier it is to instrument.  The /dynamic dispatching/ of the
object-oriented interpreter allows the effortless addition of the
=facet= value.  In all the other cases, /dynamic scoping/ was
prescribed.  We also saw that /dynamic typing/ was required at least
in the ‘context object’ solution to the extra ‘pc’ argument.

Intuitively, it makes sense.  We want different instances of the same
names (=interpretProgram=, =ref=, =rules=) to have different
behaviors, depending on context.  This is exactly what dynamic
dispatching is for: the same method slot can refer to different
implementations, depending on the actual instance of the receiver.
This is also the difference between lexical and dynamic scoping: the
former binds free names to their static surrounding context at
definition time, while the latter binds free names to their caller’s
context at runtime.

In a language with dynamic scoping the interpreter should be a breeze
to instrument.  That is the focus of the [[Lisp][Lisp variations]].

** Lisp
[[file:lisp/pm.lisp]]

Common Lisp offers both lexical and dynamic scoping of variables.  We
implement the pattern-matching standard interpreter by defining the
=*rules*= object to be dynamically scoped (by using =defparameter=).
We follow the Common Lisp convention of using stars to surround a
dynamically-scoped name.

#+BEGIN_SRC lisp
  (defparameter *rules*
    `((c . ,(lambda (s env node) ...))

      (ref . ,(lambda (s env node) ...))))

  (defun eval-node (store env node)
    (let ((f (lookup (car node) *rules*)))
      (funcall f store env node)))

  (defun eval-program (AST env store)
    (eval-node store env AST))

  (eval-program
   '(app (fun "x" (deref (v "x")))
         (ref (c 42)))
   '() '())
#+END_SRC

Instrumentation is then effortless.  First we define facet-specific
rules by extending the basic rules objects (=append ... *rules*=).

#+BEGIN_SRC lisp
  (defparameter *facets/rules*
    (append
     `((ref . ,(lambda (s env node)
                 ...
                 (mk-facet *pc* v1 bottom)
                 ...)))
     ,*rules*
     ))

  (defparameter *facets/application-rules* ...)
  (defparameter *facets/deref-rules* ...)
  (defparameter *facets/assign-rules* ...)
#+END_SRC

Note the reference to the free dynamic variable =*pc*=.  In the entry
point to facet evaluation, we override the standard rules with the new
ones.  We also declare the =*pc*= argument to be dynamically scoped
inside this call, using =(declare (special *pc*))=.

#+BEGIN_SRC lisp
  (defun facets/eval-program (AST env store *pc*)
    (declare (special *pc*))
    (let ((*rules* *facets/rules*)
          (*application-rules* *facets/application-rules*)
          (*deref-rules* *facets/deref-rules*)
          (*assign-rules* *facets/assign-rules*))
      (eval-program AST env store)))

  (facets/eval-program
   '(app (fun "x" (deref (v "x")))
         (ref (c 42)))
   '() '() '(1))
#+END_SRC

So as anticipated, dynamic scoping is an adequate solution to the
issues we ran into in the JavaScript variations.  But dynamic scoping
is not available in all languages.  As a fallback, we saw that a
language with mutable global variables could emulate dynamic scoping.
This begs the question: in an immutable language without dynamic
scoping, how do we deal with the problem of modular instrumentation?

Furthermore, we also saw that dynamic typing was useful in the
‘context’ object case.  What if we want to benefit from the guarantees
provided by static types?  Is the modular instrumentation still
feasible?  Is it cumbersome to write?  That is the focus of the
[[Haskell][Haskell variations]].

** Haskell
Building scaffolding with languages features has the following
advantages:
+ No extra syntax or rewriting program required
+ In statically-typed Haskell, the scaffolding is type-checked

Downsides:
- The scaffolding might is seldom straightforward
- Extension + overriding of existing definitions leads to very complex
  code

Extending the syntax is the same, with pros and cons inversed:
- Extra syntax and rewriting program required
- Rewritten program is type-checked, but transformation must be proven
  correct

Advantages:
+ Lightweight syntax is straightforward to use
- Overriding it still awkward to read

*** Building scaffolding with language features
**** Monadic interpreters
The monadic interpreter is mostly taken from [[http://homepages.inf.ed.ac.uk/wadler/papers/essence/essence.ps][Wadler]].

- [[http://web.cecs.pdx.edu/~mpj/pubs/modinterp.html][Monads Transformers and Modular Interpreters]]
  + [[http://www.cas.mcmaster.ca/~kahl/FP/2003/Interpreter.pdf][Haskell implementation]]

**** Either data type
See [[file:hs/extend-types/Extension.fail.1.hs]].  Types are extended like
so:

: data FacetTerm  = Facet Principal FacetTerm FacetTerm | BaseTerm Term

***** What does work
- Maximum reuse from the file ‘Base.hs’
- Able to execute `term0` and `term1`

***** What fails
- `term2` gives a type error:
    “Couldn't match expected type `Term' with actual type `FacetTerm'”

: term2 = (Lam "y" (Facet 0 (BaseTerm (Lam "x" Bot)) (BaseTerm Bot)))

- Also, have to wrap Base.Term values with the BaseTerm constructor

***** What I wanted
- `eval term2` gives the same result as when using Extended.eval.

- The raw term1 and term2 should type without wrapping values.

***** Why it doesn’t work
A FacetTerm is either a Facet or a BaseTerm.  A Facet can contain
FacetTerms (and hence BaseTerms), but since BaseTerms are just Terms,
they cannot contain Facets.

***** Conclusion
What we really want is to insert the constructor `Facet` into the
existing data type `Term`.

**** Type classes
Another suggestion by Rémi.

#+BEGIN_SRC haskell
interp :: Dom d => Term -> d
interp (Add l r) = myAdd (interp l) (interp r)

class Dom d where
  myAdd :: d -> d -> d

instance Dom Int where
  myAdd = +

instance Dom OddOrEven where
  myAdd = xor
#+END_SRC

Here you must generalize the interpreter, to accomodate multiple
domains.  But at least the generalization is done using types: the
overhead is minimal.  Though you still need to have indirect calls.

> Ismael: Some disadvantages of this approach are discussed in the
Open Data Types paper, in Section 6.4.

***** Multi-param types classes
#+BEGIN_SRC haskell
class Eval term value where
  eval :: term -> value

#+END_SRC

Becomes quite complicated rapidly.  First you need an extension, then
you quickly run into typing issues that are not worth the flexibility
offered by this strategy.

**** Data types à la carte
From Swierstra, 2008.

[[file:hs/extend-types/Extension.swierstra.hs][A first attempt]]

[[file:hs/InterpreterALC.hs][Another one, with Ismael]]

[[file:hs/ALC-Lambda.hs][Lambda calculus with references and bottom]]

[[file:hs/ALC-Lambda-Facets.hs][Lambda calculus with faceted evaluation]]

[[file:hs/ALC-Facets-Flow.hs][Lambda calculus with faceted evaluation and FlowR tainting]]

Overview of this scaffolding:

Pros:
+ Allows type-checked extension of terms

Cons:
- Quite hairy
- Cannot change the resulting value with using the same approach for
  the value type, which would be even /hairier/.

**** Data types  à la carte (2015)
Re-reading Swiestra in 2015, my brain parses it much better.

- Coproduct is just disjoint-union.
- Construction of values of the coproduct type are unwieldy, hence the recourse
  to type classes that will automatically call the right injection into the
  coproduct based on the type of the expression.  Expression types are required,
  otherwise the compiler cannot find the right injection.

  The use class constraints and instances to automatically figure out the right
  injection is quite clever.
- Can’t we derive functors, for the coproduct, since they seem rather
  straightforward?  We can.  Probably `deriving Functor` was not available in
  Haskell in 2008, when Swierstra wrote it.
- We can give a larger type signature than necessary, since the compiler cannot
  infer it for us.  =Add :+: Val :+: Add :+: Val= is perfectly acceptable.  In
  practice, this is harmless.
- While we can easily extend the language and compose languages using this
  approach, modifying behavior requires defining a new algebra.  Can we re-use
  parts of a previous algebra?  There is no clue as to how to achieve that in
  this paper.
- Having terms appear in the signature to programs gives you guarantees for
  free.  As illustrated in the paper, a program of type =Term Recall Int= (\sect6)
  will never modify the value of the memory cell.  Also the topic of \sect7, and
  related work on type systems for effects.

**** Implicit arguments
A simple idea: implicit arguments can replace global variables.

Unfortunately, describing an override of the base eval function is still an
issue.  Maybe use type classes?

See [[file:hs/implicit][implicit]].


*** Extending the syntax
See [[file:hs/transform/notes.org]]

[[file:hs/transform/tests/2/LC.hs][Lambda calculus with FlowR instrumentation]]

- Cannot override existing definitions (like one would do with aspects)
- Extending the monadic stack is best done with scaffolding, though
  obliviousness is lost

** Modular monadic interpreters
*** The giants
There’s a body of work on monadic interpreters in functional programming
languages built around monads, dating from the ‘90s.

Starting with Eugenio Moggi lectures at Edinburgh in 1990, then followed by
Wadler, Steele, Hudak and Jones.

- Eugenio Moggi.  An abstract view of programming languages. 1990
- Philip Wadler.  The essence of functional programming. 1992
- Guy Steele.  Building interpreters by composing monads. 1994
- Sheng Liang, Paul Hudak, Mark Jones.  Monad Transformers and Modular
  Interpreters. 1995
- Luc Duponcheel.  Using catamorphisms, subtypes and monad transformers for
  writing modular functional interpreters.  1995
- David Spinoza.  Semantic Lego (thesis).  1995

Then there is the, more recent, use of the free monad to build DSLs and
interpreters in functional languages.  Swierstra reports that free monads were
well-known in category theory, but unfamiliar to functional programmers.

- Wouter Swierstra.  Data types à la carte.  2008
- Tom Schrijvers, Bruno C.d.S. Oliveira.  Functional Pearl: The Monad Zipper.
  2010
- Oleg Kiselyov, Amr Sabry, Cameron Swords.  Extensible Effects -- An
  Alternative to Monad Transformers.  2013
- Rúnar Óli.  Compositional application architecture with reasonably-priced
  monads (Scala Days talk).  2014

On the other side of the fence, in OO-land, the go-to solution is a mix of
modules and design patterns.

- Robert Bruce Findler and Matthew Flatt.  Modular Object-Oriented Programming
  with Units and Mixins.  1998
- Bruno C.d.S. Oliveira.  Modular Visitor Components -- A practical Solution to
  the Expression Families Problem.  2009
- Brudo C.d.S. Oliveira, William R. Cook.  Extensibility for the Masses --
  Practical Extensibility with Object Algebras.  2012

*** The bigger picture
In the solutions from the functional programming world, the idea seems to
revolve around reifying the computation (the target program of an interpreter),
and write different interpreters.  In other words, to achieve modularity, one
must turn behavior into data: data has no effect unless it is executed by some
interpreter and turned into an actual computation.

So, essentially we are writing programs in a DSL of which we know one
interpreter (hopefully).  This is high-level, programming the specification.
Then we are free to write other interpreters for the same program, to change its
meaning slightly.

The connection to DSLs of course brings works from other communities to the
table.

Viewing the interpreter as data is something that is free in Lisps.  The code
can always be seen as a data structure that is easily parsed.  If turning your
code to data gives you the greatest modularity, then homoiconic languages have
a clear advantage of not requiring a parser.

* Discussion
In object-oriented languages, we would have used the mechanisms of
inheritance and overriding (with the keyword =super=) to solve this
problem.  In prototype-based languages, we would have used delegation.
[Not necessarily ... in JavaScript we get by using only objects as
dictionaries, and functions.]

#+BEGIN_SRC java
  interface Term {
    Value interp();
  }

  class Constant implements Term {
    int i = 0;
    Constant(int i) { this.i = i; }
    Value interp() { return new Number(this.i); }
  }

  interface Value<T> {
    T get();
  }

  class Number implements Value<Integer> {
    int i;
    Number(int i) { this.i = i; }
    Integer get() { return this.i; }
  }

  class Plus implements Term {
    Term t1, t2;
    Plus(Term t1, Term t2) { this.t1 = t1; this.t2 = t2; }
    Value interp() { return plus(t1.interp(), t2.interp()); }

    Value plus(Number a, Number b) {
      return a + b;
    }
  }

  class Constant42a extends Constant {
    @override
    Value interp() { Value r = super.interp(); return new Number(r.get()); }
  }

  class Constant42b extends Constant {
    @override
    Value interp() { this.i *= 42; return super.interp(); }
  }
#+END_SRC

#+BEGIN_SRC javascript
  var terms = {
    Constant: function(i) { return { type: 'Constant', i: i }; },
    Plus: function(a, b) { return { type: 'Plus', a: a, b: b }; },
  };

  var values = {
    Number: function(i) { return { type: 'Number', value: i }},
  };

  var rules = {
    Constant: function(term) {
      return values.Number(term.i);
    },

    Plus: function(term) {
      return dispatch(plus, interp(term.a), interp(term.b));
    },
  };

  var plus = {
    'Number-Number': function(a, b) {
      return a.value + b.value;
    },
  }

  function dispatch(functions, arg1, arg2) {
    functions[arg1.type + '-' + arg2.type](arg1, arg2);
  }

  function interp(term) {
    rules[term.type](term);
  }
#+END_SRC

[Examples: modify + to add 2 (change return value); log calls (add side effect)]

** Instrumenting the interpreter vs. rewriting the code
An alternate way of changing the meaning of programs is to rewrite
them.  Instead of modifying the interpreter for the base language, we
express the instrumentation as a transformation on the AST of target
programs.  This transformation typically produces a larger AST, but an
AST of the base language.  Hence, the base interpreter needs no
modification.

Two questions can be raised about such transformations:
1. Can we always find a transformation corresponding to an extension
   of semantics, and vice versa?
2. Are both approaches completely equivalent for all intents and
   purposes?  In the case of faceted evaluation, the analysis would
   now be a part of the program run by the base interpreter, meaning
   that user code might have access to the state private to the
   analysis, such as the program counter.  Can we devise a
   transformation that guarantees that user code will never interfere
   with the analysis?

The answer to the first question is probably yes.  If we have an
instrumented interpreter, and the language it targets is powerful
enough, then we can always write the instrumented interpreter itself
in this language.  From then on, we can always feed it a program of
the instrumented language.  The other way around, it seems we can
always extract the transformation and put it in the interpreter.

Are they completely equivalent?  Theoretically, they might just be.
But in practice, by instrumenting the interpreter we gain a layer of
insulation, since JavaScript is only weakly reflexive; programs do not
have access to the interpreter’s running state.  On the other hand,
transformations are easier to compose and easier to prove correct.

The [[http://www.cs.cornell.edu/Projects/polyglot/][Polyglot project]] is an extensible Java compiler front-end used for
writing Jif, among other extensions.  It turns an extended AST into
standard Java bytecode after several passes, all of which can be
extended with low overhead.

** Questions [7/7]

- [X] What difficulties arise from having a monadic interpreter as a
  base?  Is it fundamentally different?

  A: When the interpreter is monadic, it’s easy to change the type of
  the monad to pass the Program Counter.  When the interpreter is not
  monadic, it becomes much harder to modify.

- [X] Why can’t you follow Wadler or Steele and put the whole
  instrumentation inside a monad?

  A: I can’t figure a way to make this work.  See [[Facets as a monad]].

- [X] Writing the interpreter in a monadic style can be considered a
  form of modularization.  So, a monadic interpreter is already “open”
  to extension in a certain way.  Why not assume that the interpreter
  is already amenable to instrumentation; assume it already defines
  hooks for instrumentation purposes?

  A: Ideally, the way we implement the interpreter should not impact
  the implementation of the instrumentation.  The interpreter could be
  monadic, it could include hooks.  Or not.  The implementation
  choices should not influence the implementation choices of the
  instrumentation.

  What this means is that we should decouple the implementation of the
  instrumentation from the implementation of the interpreter.  We
  should treat the latter as a black box.  The instrumentation
  implementer should not break the barrier of abstraction and look at
  how the interpreter code works for writing her own.  Then, the only
  solution is for the two parties to agree on an extension interface
  for the interpreter.

- [X] How to define such an interface modularly?

  A: Joinpoints already provide “hooks” for the instrumentation.  But
  not all joinpoints are of interest for the instrumentation.  The
  interpreter must provide hooks like “around ref”.  An elegant way to
  provide them is to use AOP.

  We get the following three components; each line interacts with the
  one above only (low coupling).

  + interpreter, written in language X
  + extension interface, defined by exposing joinpoints
  + instrumentation, written in language X, with pointcuts on the
    extension interface

- [X] How is this not already solved by Open Data Types and Open
  Functions?

  A: Open Functions does not give you `proceed`.

- [X] What happens when the original interpreter and the extensions
  evolve in time?  How can we keep the two interpreters in sync, and
  minimize the overhead?  Especially as AOP introduces a strong
  coupling in the form of pointcuts.

  A: This is out of scope.  Evolution implies changes to the code.
  Changes can be of two natures:

  - a simple refactoring: code is moved around, things are renamed,
    functionality is moved in or out of functions.  The point is, the
    semantics of the program remain the same.

    If you change the exposed interface, you break clients.  If you
    move code or rename functions, you break your interface to
    pointcuts, and you break aspects.

    The whole point of modularity is to avoid global changes when
    making local changes.  At some point though, the cost of retaining
    the same interface while making lots of internal changes will be
    too great.

  - a change in the meaning of the program.  Rien ne va plus; all bets
    are off.

  Since we are presenting just a few ways to write the interpreter and
  its instrumentation, we cannot guarantee anything when these
  patterns are changed.  Nor can we say anything about a /different/
  interpreter.

- [X] What if we don’t have access to the original interpreter’s
  source code.  This is a legitimate scenario.  How do instrument the
  running code then?

  This maybe related to the path taken by Ansaloni and Binder.
  “Blind” instrumentation.  But really, this is out of scope.

** Downsides to the monadic interpreter approach for modularity
*** Explicit use of monads
You have to explicitly write the interpreter to return monads instead
of raw values.

Though one could argue that it’s just a clearer way to write an
interpreter from the start.

With Haskell’s syntactic sugar for unit and bind, the cost is not that
great (though there’s still some mode switching required “I have to
`return` because it expects a monad”).

*** The `lift` uglyness
When using multiple State monad transformers, you must use `lift` to
access the state you want from the monad stack.  The stack order
/matters/, but it should not.

Ismael tells me there are workarounds, a library that allows you to
name the monad transformers and access them by name instead of using
lift (I guess it does the lifting for you).

If there are no hidden costs or restrictions, then this is not a
downside anymore.

*** Limitations for extending cases for pattern matching
You can’t just extend functions with additional cases, since
functions are closed at definition time.

For instance, adding the evaluation of a new `Facet` AST node requires
adding a new case to `interp`.

#+BEGIN_SRC haskell
interp (Facet p t1 t2) e =
  do vH <- interp t1 e
     vL <- interp t2 e
     return (FacetV p vH vL)
#+END_SRC

But this can’t be done in another file, even though Haskell allows you
to write non-exhaustive functions ...

Using Ismael’s AOP library for Haskell, you can work around it, though
it requires some additional rewriting of the code for technical
reasons (supposedly this could be hidden by sugar).

In any case, you lose the ability to write your extensions *like you
would write* the original code.

This problem seems solved by [[http://web.cecs.pdx.edu/~mpj/pubs/modinterp.html][Monads Transformers and Modular
Interpreters]].  It is also definitely solved by [[http://www.andres-loeh.de/OpenDatatypes.pdf][Open Data Types and
Open Functions]].

The latter also allows you to think of open data type extensions as
one monolithic data type (semantically equivalent).  Except when
textual order of constructors matter.

*** Can’t easily extend the data types either
To add a new AST node, you need to:

1. Extend the `Term` data type
2. Add a new case to `interp` for this new Term

Here again, the data type definition is closed.  No reflection
mechanisms to extend it?  Are there workarounds?

[[http://web.cecs.pdx.edu/~mpj/pubs/modinterp.html][Monads Transformers and Modular Interpreters]] define OR types for this
purpose.  It does not feel very natural to write, but at least if the
mechanism is there, we could hide it with sugar.

[[http://www.andres-loeh.de/OpenDatatypes.pdf][Open Data Types and Open Functions]] solves the problem in a modular
way: extensions to the data type can happen in other modules.

The “inter-types aspects” of AspectJ allow modifying the static
structure of an existing class, by extending its members or methods.
Basically, the same thing is needed here.  If the AOP library for
Haskell provided inter-types aspects, then it’s done.

** Facets as a monad
If the instrumentation of the interpreter was entirely confined to a
monad, it would be the epitome of modularity (bar the two first points
above).

Can this be achieved?  Can we embed the faceted evaluation inside a
monad?

The `FacetMonad` could hold a full interpreter with duplicated
environment, store and program counter.  But that would mean
duplicating the standard rules.  And that means having a second full
interpreter with faceted evaluation ... so you just duplicate, but in
very a convoluted way.  The result would not be modular, and harder to
understand than straightforward duplication.

** Opening `interp`
Suggested by Rémi.

Instead of opening each case individually, `goRef` or `goIf` and so
on, we can open `interp` directly.

#+BEGIN_SRC haskell
interp t e = goInterp # (t,e)

goInterp (Bot,e) = return Bottom
goInterp ((Con i),e) = return (Constant i)
...
#+END_SRC

If the pointcut language has an `if` construct that can match on
arguments, we can dispatch advices depending on the term argument.

I > We can use the RequirePC special pointcut. I added an example in the
I > source code.

#+BEGIN_SRC haskell
deploy (aspect (and (pcCall goInterp) (match (Ref t))) goIfAdv)
#+END_SRC

Our pointcut language does not allow us to match like this, but at the
very least we could contain advices in a `goInterpAdv` function.

#+BEGIN_SRC haskell
deploy (aspect (and (pcCall goInterp)) goInterpAdv)

goInterpAdv proceed args@((If cond thn els), e) =
  -- code for goIfAdv

-- fall through
goInterpAdv proceed = proceed
#+END_SRC

** Comments
(Ismael): I think we should enumerate all the required changes along
their nature: data type extension, new case for functions, etc. Doing
a diff on the LC-standard and LC-facet files yields the following:

- Term is extended with the Facet variant
- Value is extended with the FacetV variant
- instance of Eq Value is updated with the FacetV variant
- instance of Show Value is updated with the FacetV variant
- Modification of type M (new StateT)
- New runM function  (caused by the change to M)
- New case (Facet p t1 t2) added to interp
- Case (Ref t) is modified by what it looks like an around advice
- New case (Facet p t1 t2) added to helper function deref
- In helper function assign, Case (Address a) is modified by what it looks like an around advice
- New case (FacetV p vh vl) added to helper function assign
- New case (FacetV p vh vl) added to helper function apply

I propose that the contribution of the paper is a comparison or
classification of the kinds of extensibility that are desirable for a
modular instrumentation of a monadic interpreter. For extending data
types we can use the Either approach of Hudak (or maybe both
approaches are useful), and for adding cases we use AOP. Then we
discuss the benefits/drawbacks of this approach vs the Open Data Types
and Open Functions.

What we bring to attention is that AOP is (unsurprisingly) helpful to
define "open functions". Whereas the approach of Open Data... is less
expressive because it lacks a pointcut language (or something along
these lines).

*** Comparing LC-facets with LC.hs
- Term is extended with the Facet variant
- Value is extended with the FacetV variant
- instance of Eq Value is updated with the FacetV variant
- instance of Show Value is updated with the FacetV variant
- New type M, now using AOT
- New runM function, where all aspects are deployed
- New case (Facet p t1 t2) added to helper function deref
- In interp: Case (Ref t) is refactored adding a goRef function which is open to weaving
- Similar change for deref function.
- Similar change for assing function.
- It would be more symmetric if all introductions of # were at the same level, e.g. at the interpreter.
- Same change for apply

Ismael >

A conclusion for this simple analysis is that to add new cases to
interp we also need to make it advisable, following Rèmi's suggestion
outlined above. In other words, if we allow for new variants to Term,
we *need* an open interp. I think this is not mutually exclusive with
the goFoo pattern, because when extending some behavior we actually
require access to the default implementation (e.g. in the (Ref t)
case, we need proceed to refer to the goRef default implementation).

Maybe this highlights the need for an extension of the pointcut
language: to be able to target a function with a specific case, while
still being able to refer to the default implementation by using
proceed. Actually this can be done using RequirePC (See file
LC-ismael.hs):

*** Using RequirePC to advice a particular case of goInterp
#+BEGIN_SRC haskell
-- Require PC for Ref case
refPC :: Typeable1Monad m => RequirePC m (Term, Environment) b
refPC = RequirePC $ return (\ jp -> case unsafeCoerce jp of
                               (Jp _ _ ((Ref t, _))) -> return True
                               _ -> return False)

-- note the unsafeCoerce is actually safe because... *read TAOSD paper Section 4.1*

-- i13n
runM :: M Value -> ProgCounter -> Store -> ((Value, ProgCounter), Store)
runM m pc s = runIdentity (runStateT (runStateT (runAOT prog) pc) s)
 where prog = do
           -- deploy (aspect (pcCall goRef) goRefAdv)       -- i13n
           deploy (aspect (pcAnd (pcCall goInterp) refPC) goRefAdv)
           deploy (aspect (pcCall goDeref) goDerefAdv)   -- i13n
           deploy (aspect (pcCall goAssign) goAssignAdv) -- i13n
           deploy (aspect (pcCall goApply) goApplyAdv)   -- i13n
           m
#+END_SRC

To me, the "epitome of modularity" would being able to something
like what is sketched in LC-ismael-ideal.hs. It seems this can be
achieved using some kind of generative programming. See comments in
that file for the issues I've encountered so far...

** Naming functionality is vital
[On the granularity of advices: the function is the boundary.  Messing
around inside functions is too brittle; you break the barrier of
abstraction.]

There’s this very strong tentation to over-factor programs.

Syntactical repetition is easy to spot, but it does not always
indicates a functional duplication.

Let’s take an example.  These two pattern cases of the `eval` function
have strong syntactic and functional similarities.

#+BEGIN_SRC haskell
eval (Ref t) =
  eval t >>= \v ->
  gets store >>= \s ->
  let addr = length s in
  puts inStore ((addr,v) : s) >>
  return (Address addr)
#+END_SRC

#+BEGIN_SRC haskell
eval (FRef t) =
  eval t >>= \v ->
  gets store >>= \s ->
  let addr = length s in
  gets progCounter >>= \pc ->
  let fv = createFacetValue pc v Bottom in
  puts inStore ((addr,fv) : s) >>
  return (Address addr)
#+END_SRC

In fact, here is the second function again, where the lines identical to
the first function are rendered as a dot.

#+BEGIN_SRC haskell
eval (FRef t) =
  .
  .
  .
  gets progCounter >>= \pc ->
  let fv = createFacetValue pc v Bottom in
  puts inStore ((addr,fv) : s) >>
  .
#+END_SRC

Three lines are changed: two are completely new, and one is only
slightly altered.  Actually, there’s only one identifier that changes
in the third line, so we can go further, and highlight changes at the
symbol level.

#+BEGIN_SRC haskell
eval (FRef t) =
  . . . . .
  . . . . .
  . . . . . .
  gets progCounter >>= \pc ->
  let fv = createFacetValue pc v Bottom in
  . . ....fv. . .. .
  . .. ..
#+END_SRC

Now we have fully isolated the syntactic changes made in the second
version.

However, how do these syntactic changes relate to semantic changes?
Different source codes, or ASTs, can evaluate to the same outcome (see
[[A digression on injective programming]]).  If I change a symbol in the
program, do I change its meaning?  Maybe!  It depends; not only it
depends on the symbol, but also on its surrounding context.

The temptation I am too often victim of is to try to eliminate the
syntactic duplication symbolized by dots in the examples above.

Instead of repeating myself, I would like to write:

#+BEGIN_SRC haskell
eval (FRef t) = same as `eval (Ref t)` except for
  [..]
  gets progCounter >>= \pc ->
  let fv = createFacetValue pc v Bottom in
  . . ....fv. . .. .
  --
#+END_SRC

Here `[..]`, `--` and `.` represent any number of lines, any line, and
any character respectively.

Such conciseness.  Much economy.  So fragile.

This new version is brittle; any syntactic change (again, not
necessarily semantic), can alter the semantics of the second version.

Here is why.  This first function is an innocuous ‘plus’.

: plus a b = a + b

The second adds three numbers, but reuse syntax from the first.

: plus3 a b c = . . . + c

Now what happens if the first function is rewritten to one the
/semantically equivalent/,


#+BEGIN_SRC haskell
plus = (+)
plus a = (+) b
plus = \a -> \b -> a + b
plus a b = 0 + a + b
plus a b = 0 + a + 0 + b
...
#+END_SRC

Each of these are valid ways to declare a function plus, and all will
(or should) compile to the /exact same/ binary code.  But the `plus3`
function will be ill-defined for all these variants.

This problem stems from the fact that there are infinitely many ways
to write a program that produces an addition.

The intent behind the syntactic ellipses in `plus3` is really
semantic.  What is meant is,

: [| plus a b |] = [| a |] + [| b |]

(where ‘[|’ and ‘|]’ are brackets of denotational semantics).

We know that `plus` is standard addition between two values (hopefully
numbers).

And we want to define `plus3`, the addition between 3 values.  We
write

: [| plus3 a b c |] = [| a + b + c |]

But this is clearly repeating ourselves; the addition between two
numbers was already defined.  So we then write

: [| plus3 a b c |] = [| ...  + c |]

which is to say, “I want the meaning given by the elided syntax.
Explicitly,

: [| plus3 a b c |] = [| [| a + b |]  + c |]

Except that I cannot reuse the meaning of the syntax as one piece
until I *name* it.  Which I did, in this example; so that I can write

: [| plus3 a b c |] = [| [| plus a b |]  + c |]

But in the Ref/FRef example, the parts I want to reuse have no name.
Is that it?  Is that all I’m lacking, a name for the semantic action I
need to reuse?

Once `plus` has a name I can reuse in the definition of `plus3`, I
don’t care for syntactic changes in its operational definition.  So
long as its /semantics/ stay the same, the name will be used to /mean/
the addition of two numbers.

The name introduces a binding for a meaning.  In a way, it solves the
non-injectivity of semantically-equivalent syntax programs.  Since by
using names we refer directly to meanings.  It does not matter which
alias we use to refer to a particular meaning, like `plus` or `(+)`,
so long as they refer to the addition between numbers.

# The following was added on [2014-09-02 mar.].

The distinction made by [[file:~/Archim%C3%A8de/Th%C3%A8se/notes/quotes.org::*On%20the%20distinction%20between%20program,%20procedure%20and%20function][Steele and Sussman]] between program, procedure
and function sums the problem up.

As the authors show, the program is not the function without lexical
scoping.  Using ‘...’ for copy pasting the program text in the hope of
getting the same functionality is hopeless.  The program only makes
sense in its entirety, not piece by piece of it.

Naming the functionality is therefore the most important step to
reusability.

# [2014-09-03 mer.]

Olin Shivers has [[https://blogs.janestreet.com/whats-in-a-name][more to say]] on the importance of naming things.
Names provide distal access.  Without a name, you cannot refer to a
functionality; you have to repeat it.  With a name, now you can reuse.

As he points out, echoing Steele and Sussman, lexical scoping is
evident when you think about names.  When writing a function with free
variables (names), you /need/ the names to refer to the same thing
when executed as they did when the function was defined.

I would also add that when we make functions first-order in a
language, we are essentially giving the ability to name them.
Returning a function or passing it as an argument would not make sense
without being able to first give a name to the function.

Naming (or more generally, creating a symbol) seems also like the
principal mechanism for learning in the brain.  When you assimilate a
new fact, you are drawing an arrow, making connections, hence giving a
name to the fact.  See the idea of “frames” in “Society of Mind”
(Minsky).

*** A digression on injective programming
It should be evident that syntactic changes do not imply a change in
the meaning of the program.  In most languages, whitespace between
symbols is insignificant when it comes to meaning.

: [[1 + 2]] = [[1+2]]

Actually, in these languages, whitespace is discarded at the parsing
phase.  So the programs `1 + 2` and `1+2` are parsed to the same
abstract syntax tree.

: (Program (PlusSymbol (Constant 1) (Constant 2)))

If two programs differ in syntax but are parsed to the same AST, then
we should expect they evaluate to the same outcome.

But we can also consider duplication at the AST level.

: (Program (PlusSymbol (Constant 1) (Constant 2)))

: (Program (Plus 0 (PlusSymbol (Constant 1) (Constant 2))))

These two lines are similar.  Highlighting differences at the AST node
level,

: (Program (Plus 0 .))

The two ASTs are different, but the outcome is the same.

Parsing is a function from the set of source codes to the set of
abstract syntax trees.  Its properties depends on the parser we are
talking about.
- Safe to say that parsing is not injective for most languages: one
  AST can be the image of an infinity of source codes
  (e.g. extraneous whitespace).

  If we ignore comments and extraneous whitespace, is parsing
  injective?  Again, it depends on the parser and language.  We can
  easily craft a non-injective trivial parser: all source codes map to
  `Program`.  We can also be careful and construct an injective
  parser.

  The more interesting question is: what are the benefits of an
  injective parser?

- Is parsing surjective?  Probably not when considering the set of all
  ASTs for a given language.  In most languages, we can craft bogus
  ASTs that could not be the result of parsing.

  Again, what are the benefits of a surjective parser?

If parsing is bijective, then we have a clear benefit: any AST can be
mapped back to its source.

Evaluation is a function from the set of abstract syntax trees to the
set of values.  We can ask the same questions.
- Is evaluation injective?  Most certainly not.  Consider a calculator
  language.  The two programs `(Plus 1 0)` and `(Plus 0 1)` have
  different ASTs, but evaluate to the same outcome, the value ‘1’.

  For evaluation to be injective, we would have to craft our ASTs such
  that there is only one way to write a program with outcome ‘1’.  I
  guess such a language would not be very useful for computation.

- Is evaluation surjective?  Depends on the values we choose.  A
  calculator language with the natural integers as co-domain will
  surely be surjective.

‘Injective’ or ‘reversible’ programming is apparently [[http://dl.acm.org/citation.cfm?id=1366239][a thing]].  And
it’s not that useless!  We can even find traces of it in [[http://www-formal.stanford.edu/jmc/inversion.pdf][The Inversion
of Functions Defined by Turing Machines]].

Though they seem to consider (in the Janus language) a relaxed form of
injectivity.  Namely, we can still add dead code that changes the
syntax but not the semantics (switch `x1 += 1` and `x2 += 2`).  In
effect, there are infinitely many programs that evaluates to the same
value; the evaluation function is not injective.

** Fundamentals of the expression problem
We want to apply a function =f= on at least two different sets of
values, A and B.  That is, we have in fact two functions f_A and f_B but
we do not want to call them explicitly when dealing with a value of A
or B, since it is clear from the context which is to be called.

f_A(a) is redundant, and f(x) is clear enough from context if A and B
are disjoint.

How do we define such an =f= in programming languages?

In a language with pattern matching like Haskell or OCaml, the
straightforward way would be for =f= to hold the definitions of both
f_A and f_B.

#+BEGIN_EXAMPLE
   +- a
f -|
   +- b
#+END_EXAMPLE

#+BEGIN_EXAMPLE
f x | x in A = ...
    | x in B = ...
#+END_EXAMPLE

When the compiler encounters the call =f x=, if it knows that x is in
A or B, it can replace f by the f_A or f_B appropriately.  If the type
of x cannot be determined, then a runtime test can be inserted.

In a language without pattern matching, you would use runtime tests
anyway.

#+BEGIN_EXAMPLE
f = if (isA x) ...
    elseif (USB x) ...
#+END_EXAMPLE

Runtime tests costs CPU time, but the predicate tests for set
membership can be arbitrary; e.g., test if x is a prime.  Thus runtime
tests can be more expressive than types.

In a language with objects and dynamic dispatching, we would inverse
the relationship between f and the sets A and B.  The definition of f
is now split.

#+BEGIN_EXAMPLE
a
|- f

b
|- f
#+END_EXAMPLE

#+BEGIN_EXAMPLE
a = object
  method f = ...

b = object
  method f = ...
#+END_EXAMPLE

When the compiler encounters the call =x.f= (the notation is also
inverted, suitably), it calls the method f on the object x.  Again, if
it can be determined that x is an A, then f_A can be substituted for f
(and respectively for B).

At runtime, we can dispense of tests to know if a is an A or if b is a
B; what matters is that x has the method f, regardless of its
membership to a specific set.  If x is a prime, then it bears its own
specific version of f, f_prime.

The object decomposition has one definite benefit: the function f can
be defined piece-wise, and can be extended indefinitely by just
creating new objects which answer to the message ‘f’.

These are two fundamentally opposed ways of approaching the problem.
And yet, they share the need of imposing a hierarchy between values
and functions.

In the problem, we merely wanted to be able to say “f applied to
members of A is f_A and f_B to members of B”.  A membership predicate
allows us to support arbitrary-refined sets, as well as dispatching on
more than one argument.  If we allow functions to be extended as well,
then we get the full flexibility of both approaches.

#+BEGIN_EXAMPLE
f x | isA x = ...

...

f x | isB x = ...
#+END_EXAMPLE

Interestingly, since the definition of =f= is now split, we also lose
some clarity when reading the code.  In JavaScript, there can only be
one function definition with a given name, and all calls to this
function will pass through the same function body.  It allows
programmers to write their custom dispatching and pattern matching;
those concerns are mixed, but at least the language is unsurprising.

How do you go and implement that?  [[http://www.cs.washington.edu/research/projects/cecil/www/Gud/][Predicate dispatching]] from Ernst,
Kaplan and Chambers seems like a good start.  It raises the issue of
a possible ambiguity when resolving the predicates.

If A and B are not disjoint, then what code is to be executed when f
is applied to a member of A \cap B?  Non-determinism is a possible
answer; it might be the case that, for such an x, we do not care which
f runs, as both are valid.  However, we could also raise a runtime
exception, stating that there is more than one candidate function.

And there is also the problem of determining specificity.  If B \sub A,
and x \in B, which version of f is executed by the call =f x=?
Intuitively, we’d want the more specific; but for setting on such
choices we should be guided by real-world examples.

Ideally, we’d want to detect these errors statically, but if
predicates are arbitrary, we cannot guarantee that.

See also ad-hoc polymorphism.

* Potentially related work
GADTs? Type families? HList? Oliveira’s “Generics as a library” and
thesis?

Family polymorphism?

http://www.cs.uu.nl/foswiki/pub/Center/AspectAG/paper-icfp.pdf

http://www.dblp.org/rec/bibtex/conf/haskell/StefanRMM11


# Local Variables:
# org-confirm-babel-evaluate: nil
# End:
